# -*- coding: utf-8 -*-
"""production_pilot_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14iXpsAd2Sr67wOCOQx73_ZA1VSou0R0i

# State of the Union 

QUESTIONS: why did train do better than test?
BEST Parameters: 7 max tree depth and 10,000 max # of trees is good but how come test results are still doing better?

1.   Discovered Random Forest Regression is going to be the best solution for this problem (XGboost overfits for smaller data sets)
2.   Discovered it doesnt matter if we do weekly vesus daily
3. Discovered don't loose a ton of accuracy when moving to the entire PLU codes.
4. Purpose of this notebook:
> **TEST AN ACTUAL FUTURE PREDICTION HOW IT WOULD WORK IN THE REAL WORLD**

# Implementation Notes


*   ALWAYS GET DOUBLE the past data of the differencing weeks you want. Otherwise your data will be CRAP because youll have dropped all the nulls leaving you with barely any information! so if were differencing and shifitng 8 weeks of data we need at least 16 in the initial sql query.

# Imports
"""

# Commented out IPython magic to ensure Python compatibility.
#need to import dataframe

#Colab stuffs:

#get a fast operator system
!nvidia-smi

#mount google drive
from google.colab import drive
drive.mount('/content/drive')#click on the link it provides and copy and paste that code into the authorization area

#access the OS system to work with current directories:
import os

#Imports

import warnings 
warnings.filterwarnings('ignore')
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

from math import sqrt
import statsmodels.api as sm
from sklearn import metrics
plt.style.use('fivethirtyeight')
sns.set_style("whitegrid")
sns.despine()
sns.set(rc={'figure.figsize':(15,9)})
# %matplotlib inline

#import and clean DF
df = pd.read_csv('drive/My Drive/Capstone_2/data/one_year.csv', index_col= 'invdate', low_memory=False)#all 3000 product codes

#import and clean DF
df = pd.read_csv('drive/My Drive/Capstone_2/data/one_year.csv', index_col= 'invdate', low_memory=False)#all 3000 product codes

"""# Cleaning
Functions


*   Must implement clean_df, melt, predict_df, rf_data in ORDER because they depend on the future functions not being executed yet
"""

def clean_df (df):
  
  ################ FILTERING OUT INVALID PLUs AND RETURNS #################
  print (f'Initial Shape for DF= {df.shape} \nInitial Number of Unique PLUs = {df.item.nunique()}')

  #taking out invalid PLUs (non-items)
  def non_items ():
    #Turning items that have less than 3 numbers into nulls
    df['item'] = df['item'].apply(lambda x: x if sum(char.isdigit() for char in x) > 2 else np.nan)
  
  non_items()

  #dropping any returns because we are predicting demand not returns
  df[df.ordqty < 0] = np.nan

  print (f'\nNumber of nulls after picking out invalid entries = {df.item.isnull().sum()}')
  
  #getting rid of nulls and irrelevant columns
  df.drop(columns= ['route', 'salesman'], inplace= True)
  df.dropna(subset= ['ordqty', 'item'], axis= 0, inplace=True)

  print (f'Number of after dropping nulls (should be zero) = {df.item.isnull().sum()}\n')

  ################ CREATING SOME NEW FEATURES FOR REFERENCE LATER ####################

  #Making index datetime object
  df.index = pd.to_datetime(df.index)
  
  #Creating a normalized Order Qty column
  df['norm_qty'] = df['ordqty'] / df['units']

  #creating a column for the total orders per item
  df['total_qty'] = df.groupby('item')["norm_qty"].transform('sum')

  # #USE IF YOU WANT TO MODEL REVENUE
  # #Creating a revenue column per item per day
  # df['revenue'] = df['shipqty'] * df['price']

  # #creating a column for the total revenue per item
  # df['total_rev'] = df.groupby('item')["revenue"].transform('sum')

  #Creating a label column by splitting off the first phrase from the item's description, and turning them into lower case
  df['label'] = df['desc'].str.split(",").str[0].str.lower()
  df['label'] = df['label'].str.split(" ").str[0].str.lower()

  ############## DROPPING LOW QUANTITY PLU CODES- SET TO 10 ####################
  
  #filtering out any PLUS with less than 10 orders for the entire year
  df['low_quantities'] = df['total_qty'].apply(lambda x: x if x > 10 else np.nan)
  low= df[df['low_quantities'].isnull()]
  total_low_items = low.item.nunique()

  #IF CUSTOMER WANTS A LIST OF LOW ORDERED ITEMS:
  # print ('List of all the PLUs ordered less than 10 times')
  # list(low.item.unique())

  before= df.shape[0] 
  df.dropna(subset= ['low_quantities'], axis= 0, inplace=True)
  print (f'Number of rows/entries lost in DF after dropping lowest quantity PLUs = {before- df.shape[0]}')
  print (f'Number of unique items(PLUs) dropped for low quantity = {total_low_items}\n')
  print('##########################\n')
  print (f'Shape of DF AFTER cleaning up PLUs and low quantity items = {df.shape} \nNumber of Unique PLUs AFTER clean up= {df.item.nunique()}')

  ############## CREATING PIVOT TABLES ####################
  dfday = pd.pivot_table(df, values= "norm_qty", index= "invdate", columns = "item", aggfunc=np.sum, fill_value=0)
  
  #filling in the missing days
  idx = pd.date_range('2018-08-23', '2019-08-23')
  dfday = dfday.reindex(idx, fill_value=0)
  
  #Whoops accidentally did 366 days instead of 365! Let's fix that...
  dfday = dfday[2:] #had to take away two days so the weeks would End on the final day in the data and not create a new week with only 1 day in the week


  #creating a weekly DF
  dfwkly = dfday.resample('7D').sum()

  #resetting indexes to datetime
  dfday.index = pd.to_datetime(dfday.index)
  dfwkly.index = pd.to_datetime(dfwkly.index)

  print (f'\nDaily Shape = {dfday.shape}\nWeekly Shape = {dfwkly.shape}')

  return dfday, dfwkly

dfday, dfwk = clean_df(df)

"""# Melting & Feature Engineering"""

def melt(df):
  
  ########### INITIAL MELTING ######################
  df.reset_index(inplace= True) #reset index for melt
  melt = df.melt(id_vars='index', var_name='item', value_name='sales')
  melt = melt.sort_values(['index', 'item'])
  print (f'Number of Rows in the melted dataframe: {melt.shape}')

  # ########## Basic Feature engineering- Dates ####################
  melt2 =  melt.copy()
  melt2['date']= pd.to_datetime(melt2['index']) #converting index to a datetime
  melt2.drop(columns= ['index'], inplace= True)
  num_rows = melt2.shape[0]
  
  # Extracting date features
  melt2['dayofmonth'] = melt2.date.dt.day
  melt2['dayofyear'] = melt2.date.dt.dayofyear
  melt2['dayofweek'] = melt2.date.dt.dayofweek
  melt2['month'] = melt2.date.dt.month
  melt2['year'] = melt2.date.dt.year
  melt2['weekofyear'] = melt2.date.dt.weekofyear
  melt2['is_month_start'] = (melt2.date.dt.is_month_start).astype(int)
  melt2['is_month_end'] = (melt2.date.dt.is_month_end).astype(int)

  #Basic Feature Engineering- lags, differences, and logs-- These tend to help ALOT with time series data 
  melt2['last_wk_sales'] = melt2.groupby(['item'])['sales'].shift(1)
  melt2['last_wk_diff'] = melt2.groupby(['item'])['sales'].diff(1)
  #melt2['log_sales'] = np.log1p(melt2.sales.values)# Converting sales to log(1+sales)
  melt2['2wks_sales'] = melt2.groupby(['item'])['sales'].shift(2)
  melt2['2wks_diff'] = melt2.groupby(['item'])['sales'].diff(2)
  melt2['3wks_sales'] = melt2.groupby(['item'])['sales'].shift(3)
  melt2['3wks_diff'] = melt2.groupby(['item'])['sales'].diff(3)
  melt2['4wks_sales'] = melt2.groupby(['item'])['sales'].shift(4)
  melt2['4wks_diff'] = melt2.groupby(['item'])['sales'].diff(4)
  melt2['5wks_sales'] = melt2.groupby(['item'])['sales'].shift(5)
  melt2['5wks_diff'] = melt2.groupby(['item'])['sales'].diff(5)
  melt2['6wks_sales'] = melt2.groupby(['item'])['sales'].shift(6)
  melt2['6wks_diff'] = melt2.groupby(['item'])['sales'].diff(6)
  melt2['7wks_sales'] = melt2.groupby(['item'])['sales'].shift(7)
  melt2['7wks_diff'] = melt2.groupby(['item'])['sales'].diff(7)
  melt2['8wks_sales'] = melt2.groupby(['item'])['sales'].shift(7)
  melt2['8wks_diff'] = melt2.groupby(['item'])['sales'].diff(7)
  melt2 = melt2.dropna()

  print (f'Number of Rows lost by differencing: {num_rows - melt2.shape[0]}') #how many rows did we loose from the differencing?

  melt2.set_index('date', inplace= True)  

  #Adding Holidays--- these were events discussed with the customer
  melt2["new_years"] = (melt2.index.dayofyear == 1) | (melt2.index.dayofyear >= 359)
  melt2["christmas"] = (melt2.index.month == 12) & ((melt2.index.day >= 15)&(melt2.index.day < 26))
  melt2["thanksgiving"] = (melt2.index.month == 11) & ((melt2.index.day >= 20)&(melt2.index.day <= 29))
  melt2["farm2table"] = (melt2.index.month == 7) & ((melt2.index.day >= 1)&(melt2.index.day <= 15))
  melt2["memorial_day"] = (melt2.index.month == 5) & ((melt2.index.day >= 18)&(melt2.index.day <= 28))
  melt2["back_to_school"] = (melt2.index.month == 8) & ((melt2.index.day >= 10)&(melt2.index.day <= 25))

  return melt2

print ('DF week transformation\n')
dfwk= melt(dfwk)
print ('\nDF day transformation\n')
dfday = melt(dfday)

"""# Create a Data Frame for Predictions"""

#Creating a Dataframe for predictions

def predict_df (df):

  copy = df.copy()  
  lastwk = copy.index[-1]
  new_df = copy.loc[lastwk]#create a dataframe with just the past week

  from datetime import timedelta
  new_df['new_wk'] = lastwk + timedelta(days=7) #adding a week to predict into the future

  #correcting date features
  new_df['dayofmonth'] = new_df.new_wk.dt.day
  new_df['dayofyear'] = new_df.new_wk.dt.dayofyear
  new_df['dayofweek'] = new_df.new_wk.dt.dayofweek
  new_df['month'] = new_df.new_wk.dt.month
  new_df['year'] = new_df.new_wk.dt.year
  new_df['weekofyear'] = new_df.new_wk.dt.weekofyear
  new_df['is_month_start'] = (new_df.new_wk.dt.is_month_start).astype(int)
  new_df['is_month_end'] = (new_df.new_wk.dt.is_month_end).astype(int)

  #correcting lag sales and diffs by shifting everything over by one week
  new_df['last_wk_sales'] = new_df.sales
  new_df['2wks_sales'] = new_df.last_wk_sales
  new_df['2wks_diff'] = new_df.last_wk_diff
  new_df['3wks_sales'] = new_df['2wks_sales']
  new_df['3wks_diff'] = new_df['2wks_diff']
  new_df['4wks_sales'] = new_df['3wks_sales']
  new_df['4wks_diff'] = new_df['3wks_diff']
  new_df['5wks_sales'] = new_df['4wks_sales']
  new_df['5wks_diff'] = new_df['4wks_diff']
  new_df['6wks_sales'] = new_df['5wks_sales']
  new_df['6wks_diff'] = new_df['5wks_diff']
  new_df['7wks_sales'] = new_df['6wks_sales']
  new_df['7wks_diff'] = new_df['6wks_diff']
  new_df['8wks_sales'] = new_df['7wks_sales']
  new_df['8wks_diff'] = new_df['7wks_diff']

  #Re-doing Holidays
  new_df.set_index('new_wk', inplace= True)  
  new_df["new_years"] = (new_df.index.dayofyear == 1) | (new_df.index.dayofyear >= 359)
  new_df["christmas"] = (new_df.index.month == 12) & ((new_df.index.day >= 15)&(new_df.index.day < 26))
  new_df["thanksgiving"] = (new_df.index.month == 11) & ((new_df.index.day >= 20)&(new_df.index.day <= 29))
  new_df["farm2table"] = (new_df.index.month == 7) & ((new_df.index.day >= 1)&(new_df.index.day <= 15))
  new_df["memorial_day"] = (new_df.index.month == 5) & ((new_df.index.day >= 18)&(new_df.index.day <= 28))
  new_df["back_to_school"] = (new_df.index.month == 8) & ((new_df.index.day >= 10)&(new_df.index.day <= 25))

  #dropping columns and reformatting
  new_df.reset_index(inplace= True)
  new_df.drop(columns= ['sales', 'last_wk_diff','new_wk'], inplace=True) 
  new_df.set_index('item', inplace=True)

  return new_df

#creating the prediction dataframe
df_pred = predict_df(dfwk)

#Getting data ready for RF model

def rf_data (df):

  #Dropping Columns not relevant to a random forest
  df.reset_index(inplace= True)

  #Creating x and y training sets
  x_train = df.drop(['sales', 'last_wk_diff', 'date'], axis=1)
  y_train = df['sales'].values

  df.set_index('item', inplace=True)
  x_train.set_index('item', inplace=True) 

  return x_train, y_train, df

#generate last week before cleaning the dataframe
last_week = dfwk['weekofyear'].iloc[-1]
print (last_week)

#making the data ready for a random forest
x_train, y_train, dfwk = rf_data(dfwk)

#Check to see they both have the same number of features!
assert x_train.shape[1] == df_pred.shape[1]

"""# Baseline Model"""

#mae metric
def mae(ytrue, ypred):
  return round(mean_absolute_error(ytrue, ypred), 3) 
                                    
#rmse metric function
def rmse(ytrue, ypred):
  return (round(sqrt(mean_squared_error(ytrue, ypred)), 3))

#Restablishing a Naive Baseline and a Validation Split

mean_error = []
for week in range(last_week-5, last_week+1): #setting up six weeks train/validation set
    train = dfwk[dfwk['weekofyear'] < week] 
    val = dfwk[dfwk['weekofyear'] == week]
    
    p = val['last_wk_sales'].values
    
    error = rmse(val['sales'].values, p)
    print('Week %d - Error %.5f' % (week, error))
    mean_error.append(error)
naive_error= np.mean(mean_error)
print('Mean Error = %.5f' % naive_error)

"""# Train Model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# rf = RandomForestRegressor(n_estimators=1000, max_depth= 8, n_jobs=-1, random_state=0)
# rf.fit(x_train, np.log1p(y_train))
#         
# p = np.expm1(rf.predict(df_pred))
# print (p)

p = np.expm1(rf.predict(df_pred))
print (p)

weekly_predictions = pd.DataFrame({
    'item': df_pred.index,
    'predicted': p,
})

weekly_predictions= weekly_predictions.round(2)

weekly_predictions.head()

weekly_predictions.sort_values(by= 'predicted', ascending= False)

"""# COMPARING PREDICTIONS TO ACTUAL AMOUNTS"""

dfn = pd.read_csv('drive/My Drive/Capstone_2/data/new_info.csv', index_col= 'invdate', low_memory=False)
dfm = pd.read_csv('drive/My Drive/Capstone_2/data/master.csv', index_col= 'invdate', low_memory=False)

dfn.reset_index(inplace= True)
dfm.reset_index(inplace=True)

dfn['invdate'] =  pd.to_datetime(dfn['invdate'], format='%Y/%m/%d')
dfm['invdate'] =  pd.to_datetime(dfm['invdate'], format='%Y/%m/%d')

dfn.dtypes

############## CREATING PIVOT TABLES ####################
dfnday = pd.pivot_table(dfn, values= "norm_qty", index= "invdate", columns = "item", aggfunc=np.sum, fill_value=0)
  
  # #filling in the missing days
  # idx = pd.date_range('2018-08-23', '2019-08-23')
  # dfday = dfday.reindex(idx, fill_value=0)

#creating a weekly DF
dfnwkly = dfnday.resample('7D').sum()

#resetting indexes to datetime
dfnwkly.index = pd.to_datetime(dfnwkly.index)

nxt_wk_actl = dfnwkly.iloc[:1, :]

nxt_wk_actl

nxt_wk_actl.reset_index(inplace= True)
melt = nxt_wk_actl.melt(id_vars='invdate', var_name='item', value_name='actual')
melt.head()

weekly_predictions = weekly_predictions.merge(melt, on='item')

weekly_predictions= weekly_predictions.round()
weekly_predictions.head()

print (rmse(weekly_predictions.actual.values, weekly_predictions.predicted.values))
print (mae(weekly_predictions.actual.values, weekly_predictions.predicted.values))

plt.figure(figsize=(10,6))
plt.scatter(weekly_predictions.actual, weekly_predictions.predicted, s=300, color='lightseagreen', alpha= .4, label= "Random Forest")

x = np.linspace(0,60,60)
y = x
plt.plot(x, y, 'k', linestyle='--', label= "Perfect Prediction")
plt.legend()
plt.ylim(0,60)
plt.xlim(0,60)
plt.xlabel('Actual Quantities')
plt.ylabel('Predicted Quantities')
plt.title("How well did the Random Forest Model Do?", fontsize= 18)

grouped= dfn[['item','desc', 'label']]
grouped.drop_duplicates(inplace= True)
grouped.shape
weekly_predictions2 = weekly_predictions.merge(grouped, on='item')

#Which items are doing the worst?
weekly_predictions2['diff']= np.abs(weekly_predictions2.actual- weekly_predictions2.predicted)
bad_predicts = weekly_predictions2.sort_values(by= 'diff', ascending = False)[:100]

bad_predicts

bad_predicts.drop_duplicates(subset= 'item', inplace=True)

bad_predicts.label.unique()

bad_predicts.plot(kind= 'bar', x='label',y='diff', figsize= (20,8))

bad_predicts[bad_predicts.label== 'kale']

bad_predicts[bad_predicts.label== 'eggs']

bad_predicts[bad_predicts.label== 'mushroom']

dfn.head()

dfn.set_index('invdate', inplace= True)

kale= dfm[dfm.label== 'kale']

kale.head()

kale.norm_qty.value_counts()

