# -*- coding: utf-8 -*-
"""clean_up_grocery.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T_m42AekcICJ7X9jWV9rV5C2070RRWPj

# Data Clean Up Report

â€‹

## 1. Setup & Import
"""

#Colab stuffs:

#get a fast operator system
!nvidia-smi

#mount google drive
from google.colab import drive
drive.mount('/content/drive')#click on the link it provides and copy and paste that code into the authorization area

#access the OS system to work with current directories:
import os

# Commented out IPython magic to ensure Python compatibility.
#imports
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn import metrics
sns.set_style("whitegrid")
sns.despine()
sns.set(rc={'figure.figsize':(15,9)})
# %matplotlib inline

#Turn the CSV files into data frames
armast = pd.read_csv('drive/My Drive/Capstone_2/data/armast.csv')  
artran = pd.read_csv('drive/My Drive/Capstone_2/data/artran.csv') 
armast2 = pd.read_excel ('drive/My Drive/Capstone_2/data/new_data.xlsx', 'THY_armast')
artran2 = pd.read_excel ('drive/My Drive/Capstone_2/data/new_data.xlsx', 'THY_artran')

"""##2. Data Cleanup-ARMAST


*   Columns to delete because no added value: 
>* editedby, addedby, edited, pickup (only two dates present)
*   Columns to delete because too many null values:
>* 'cshipno', 'puller', 'ponum', 'invtype'
"""

armast.info()

armast.isnull().sum()

"""### 2.1 DROPPING USELESS COLUMNS"""

armast= armast.drop(columns= ['cshipno', 'puller', 'ponum', 'editedby', 'addedby', 'edited', 'invtype', 'pickup'])
armast2= armast2.drop(columns= ['cshipno', 'puller', 'ponum', 'editedby', 'addedby', 'edited', 'invtype', 'pickup'])

"""### 2.2 CHANGING DATA TYPES"""

#Convert all date strings to date time objects
from datetime import datetime

cols= ['invdate', 'duedate', 'delivery']
for col in cols:
  armast[col] =  pd.to_datetime(armast[col], format='%m/%d/%Y')

for col in cols:
  armast2[col] =  pd.to_datetime(armast2[col], format='%m/%d/%Y')

#Changing invoice number to int64 and converting all letter numbers into NaN's--- this will help us match up the invoice numbers and merge the two dataframes
armast.invno = pd.to_numeric(armast.invno, errors='coerce', downcast='float')
armast2.invno = pd.to_numeric(armast2.invno, errors='coerce', downcast='float')

"""### 2.3 EXPLORING COLUMN DATA


*   custno could be a a useful target predictor, or way to filter the data
*   What's going on with inventory type column? I want this column to be good!
*   Invno, must be kept as a string because some have letters in them
*   First run, route, salesman, status all could be one-hot coded...
"""

print ("Column Descriptions: \n")
for col in armast.columns:
  print (f"{col} column:")
  print (f"Data Type: {armast[col].dtypes}")
  print (f"Unique Vals: {armast[col].nunique()}")
  print (f"Nulls: {armast[col].isnull().sum()} \n")

#I'm curious how many of the 72K entries over and underpaid their bill amount
armast['diff_amt_paid'] = armast.amount- armast.paid
print (f"Number of entries that overpaid bill amount: {armast[armast.diff_amt_paid > 0.0].shape[0]}")
print (f"Number of entries that underpaid bill amount: {armast[armast.diff_amt_paid < 0.0].shape[0]}")
print (f"Number of entries that paid in full(pefect payment): {armast[armast.diff_amt_paid == 0.0].shape[0]}")

#Does this correspond to the status types?? 
armast.status.value_counts()#No, it looks like status types are independent of this...

"""## 3. Data Cleanup-ARTRAN"""

artran.head()

artran.info()

artran.isnull().sum()

"""### 3.1 DROPPING USELESS COLUMN"""

#Dropping columns with too many NaNs

artran.drop(columns= ['notes', 'cool', 'coolprnt'], inplace= True)
artran2.drop(columns= ['notes', 'cool', 'coolprnt'], inplace= True)

"""### 3.2 CHANGING DATA TYPES"""

#Convert all date strings to date time objects
from datetime import datetime

cols= ['datesold']
def change_date_time(df, cols=None):
  for col in cols:
    df[col] =  pd.to_datetime(df[col], format='%m/%d/%Y')

change_date_time(artran, cols)
change_date_time(artran2, cols)

"""### 3.3 EXPLORING COLUMN DATA"""

def col_descriptors (df):
  print ("Column Descriptions: \n")
  for col in df.columns:
    print (f"{col} column:")
    print (f"Data Type: {df[col].dtypes}")
    print (f"Unique Vals: {df[col].nunique()}")
    print (f"Nulls: {df[col].isnull().sum()} \n")
    
col_descriptors(artran)

"""### 3.4 *Least amount of items ordered*
* There are about 274 items that were only ordered once
* 152 items that were ordered twice
* 132 items that were only ordered three times
* In total these items on account for .1 percent of all the invoices
"""

558/artran.shape[0]*100 #calculating the percentage of items ordered under a certain amount
item_count = artran.groupby(['item']).item.agg('count').to_frame('count').reset_index() #Counting all the items to see which were ordered least and most
item_count.sort_values(by='count')

"""### 3.5 FEATURE ENGINEERING"""

#adding columns

#Adding a column that detects a difference between order quantity and shipped quantity (not having enough stock)
artran['diff_quantity'] = artran['ordqty'] - artran['shipqty']

print ("How many times are they understocked for their orders?")
print (artran[artran.diff_quantity > 0].shape[0])

print ("any time overshipped?")
print (artran[artran.diff_quantity < 0].shape[0])

print ("how many times perfect quantity?")
print (artran[artran.diff_quantity == 0].shape[0])

"""## 4. MERGING THE TWO DATASETS"""

#Function to figure out how many rows we are loosing

merge_outer_invo = pd.merge(armast, artran, on=['invno'], how = "outer")
print (f"Outer Merge on invoice number. \n Number of rows: {merge_outer_invo.shape[0]}\n")

left_armast= pd.merge(armast, artran, on ="invno", how= "left")
print (f"Merging on ARMAST.invno. \n Number of rows: {left_armast.shape[0]}\n")

right_artran= pd.merge(armast, artran, on ="invno", how= "right")
print (f"Merging on ARTRAN.invno. \n Number of rows: {right_artran.shape[0]}\n")

####################################

merge_outer_sono = pd.merge(armast, artran, on=['sono'], how = "outer")
print (f"Outer Merge on sono number. \n Number of rows: {merge_outer_sono.shape[0]}\n")

left_armast_s= pd.merge(armast, artran, on ="sono", how= "left")
print (f"Merging on ARMAST.sono. \n Number of rows: {left_armast_s.shape[0]}\n")

right_artran_s = pd.merge(armast, artran, on ="sono", how= "right")
print (f"Merging on ARTRAN.sono. \n Number of rows: {right_artran_s.shape[0]}\n")

#see how the nulls are distributed

#merge_outer_invo.isnull().sum() 
#left_armast.isnull().sum()
right_artran.isnull().sum()

right_artran= pd.merge(armast, artran, on ="invno", how= "right")
print (f"Merging on ARTRAN.invno. \n Number of rows: {right_artran.shape[0]}\n")

right_artran2= pd.merge(armast2, artran2, on ="invno", how= "right")
print (f"Merging on ARTRAN.invno. \n Number of rows: {right_artran.shape[0]}\n")

"""## 4.2 Cleaning Up the Merged Data"""

#getting rid of insignificant null values
right_artran.drop(columns= ['seq'], inplace=True) #because 30,146 null values
right_artran= right_artran.dropna(subset=['custno', 'item', 'uom']) #deleting less than .01 of data with only 64 entries

right_artran2.drop(columns= ['seq'], inplace=True) #because 30,146 null values
right_artran2= right_artran2.dropna(subset=['custno', 'item', 'uom']) #deleting less than .01 of data with only 64 entries

#Still need to figure out what to do with the "route" & "salesman" columns... There maybe another table to reference this and fill it in.

right_artran.isnull().sum()

#Adding a profit column
right_artran['profit'] = right_artran['amount'] - (right_artran['totalcost'] * right_artran['shipqty'])
right_artran.profit.describe()

#Correct unit types
right_artran[['invno', 'sono_x', 'sono_y']]= right_artran[['invno', 'sono_x', 'sono_y']].astype('int64')
right_artran2[['invno', 'sono_x', 'sono_y']]= right_artran2[['invno', 'sono_x', 'sono_y']].astype('int64')

#reorder dataframe
right_artran = right_artran[['invdate','invno', 'item','desc','uom','units','price', 'totalcost', 'ordqty', 'shipqty','amount', 'custno', 
              'sono_x', 'sono_y', 'paid', 'duedate','days', 'status','datesold',
              'delivery', 'route', 'salesman', 'priccode','orgprice',
            'firstrun','added', 'invline']]

right_artran2 = right_artran2[['invdate','invno', 'item','desc','uom','units','price', 'totalcost', 'ordqty', 'shipqty','amount', 'custno', 
              'sono_x', 'sono_y', 'paid', 'duedate','days', 'status','datesold',
              'delivery', 'route', 'salesman', 'priccode','orgprice',
            'firstrun','added', 'invline']]

right_artran.nunique()

# creating a daily_df
daily_df= right_artran.set_index('invdate').resample('D', level= 0).sum()
daily_df2= right_artran2.set_index('invdate').resample('D', level= 0).sum()

daily_df.head()

dfwkly = right_artran.set_index('invdate').resample('7D').sum()
print (dfwkly.shape)
print (dfwkly.head())

dfmth = right_artran.set_index('invdate').resample('M').sum()
print (dfmth.shape)
print (dfmth.head())

df2wk = right_artran.set_index('invdate').resample('2W').sum()
print (df2wk.shape)
print (df2wk.head())

"""## 4.3 Exporting cleaned up file"""

right_artran.reset_index(inplace= True)
daily_df.reset_index(inplace= True)

right_artran.head()

right_artran.set_index('invdate', inplace=True)
right_artran2.set_index('invdate', inplace=True)
right_artran.head()

df_2018 = right_artran ['2018':]
df_2018.reset_index(inplace= True)

df_2018.head()

#creating one and two year data frames

one_year= right_artran ['2018-08-23': '2019-08-23']
new_info = right_artran2 ['2019-08-24': ]
#two_year= right_artran ['2017-08-23': '2019-08-23']

new_info.head()

master = pd.concat([one_year, new_info])
master['2019-08-23':'2019-08-24']

one_year.reset_index(inplace=True)
new_info.reset_index(inplace=True)
master.reset_index(inplace=True)


#two_year.reset_index(inplace=True)

#exporting the weekly data
from google.colab import files
# right_artran.to_csv(r'/content/drive/My Drive/Capstone_2/data/df_wkly.csv', index = None, header=True)
# daily_df.to_csv(r'/content/drive/My Drive/Capstone_2/data/df_daily2.csv', index = None, header=True)
#df_2018.to_csv(r'/content/drive/My Drive/Capstone_2/data/df_2018.csv', index = None, header=True)
one_year.to_csv(r'/content/drive/My Drive/Capstone_2/data/one_year.csv', index = None, header=True)
new_info.to_csv(r'/content/drive/My Drive/Capstone_2/data/new_info.csv', index = None, header=True)
master.to_csv(r'/content/drive/My Drive/Capstone_2/data/master.csv', index = None, header=True)


#two_year.to_csv(r'/content/drive/My Drive/Capstone_2/data/two_year.csv', index = None, header=True)

right_artran.shape

#Final inspection of the columns and values of the clean dataset
col_descriptors(right_artran)

