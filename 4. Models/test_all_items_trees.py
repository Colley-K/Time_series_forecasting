# -*- coding: utf-8 -*-
"""test_all_items_trees.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MTsJo-A7GIc5AIT1aMjBmRjc6u_tgTen

# TESTING TREES...

>
![alt text](https://drive.google.com/uc?id=1Xvm8HeymRaVlqs_1qdy7eBlvsS_7FDoY)

#### Purpose of this notebook:

1. to test weekly versus daily
2. to test all 3000 products not just the 100 most popular

# Implementation Notes


*   weekly sql for one years of past data from that timeframe
*   weekly forecast? how to do every sunday and thursday?

#Shifted Target Features

* Another source of feature information is what I would call shifted target information or lagged features; that is we can look a recent, or relevant, observation of the thing we are trying to predict, and use this as a feature in the model.

* The important principle is that your model data must not contain information that would not be available at the time of prediction so a one day shift is only available one day before. So your model is only predicting one day ahead. If you wanted to forecast calls for the entire year upfront, you would not have access to the one day shifted information, and so you would need to make appropriate modifications to the shift i.e. 365 day shift.

* In the introduction we formulated the business problem as trying to predict seven days ahead so I have only included shifted targets that would be available 7 days before.

* We will also examine the impact on model performance of leaving these features out of the model, as leaving out the shifted variables will make the modelâ€™s use less restricted.

#LGBM EXAMPLES BELOW

EXCELLENT EXAMPLE: https://www.jpytr.com/post/time-series-with-gradient-boosted-models/

# Setups/ Imports
"""

#Colab stuffs:

#get a fast operator system
!nvidia-smi

#mount google drive
from google.colab import drive
drive.mount('/content/drive')#click on the link it provides and copy and paste that code into the authorization area

#access the OS system to work with current directories:
import os

# Commented out IPython magic to ensure Python compatibility.
#Imports

import warnings 
warnings.filterwarnings('ignore')
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error

from math import sqrt
import statsmodels.api as sm
from sklearn import metrics
plt.style.use('fivethirtyeight')
sns.set_style("whitegrid")
sns.despine()
sns.set(rc={'figure.figsize':(15,9)})
# %matplotlib inline

#import and clean DF
df2 = pd.read_csv('drive/My Drive/Capstone_2/data/100_items.csv', index_col= 'invdate', low_memory=False)#100 highest revenue items to test models
df = pd.read_csv('drive/My Drive/Capstone_2/data/one_year.csv', index_col= 'invdate', low_memory=False)#all 3000 product codes
df3 = pd.read_csv('drive/My Drive/Capstone_2/data/two_year.csv', index_col= 'invdate', low_memory=False)#all 3000 product codes

"""## Testing all product codes


*   First I need to filter out any item codes that are delivery based logs and not actual items
>* Will do this by catching any item string with less than 3 numbers which indicate its something like "SUGARTAX" or "BOTLEDEL"
*   Next I need to drop any items that haven't been ordered more than 10 times
"""

#Counting Null values in df
print (df.isna().sum())

#getting rid of irrelevant columns
df.drop(columns= ['route', 'salesman'], inplace= True)

#Getting rid of any non-item codes 
#filtering out the item codes with 2 or less numbers because these indicate non-plu codes
def count_numbers (df, col):
    
    #Turning items that have less than 3 numbers into nulls
    df[col] = df[col].apply(lambda x: x if sum(char.isdigit() for char in x) > 2 else np.nan)

    #Dropping the bad items from the data frame
    df.dropna(subset= [col],axis=0, inplace=True)

count_numbers(df, 'item')

#Dropping the negative values because they represent returns and we are only measuring demand/ returns is a different problem

#counting negative values
print (f'Number of Negative Order Quantities Values: {np.sum((df.ordqty < 0).values.ravel())}') 
print (f'Number of credits given to customer: {np.sum((df.price < 0).values.ravel())}') 

#dropping the return or negative value rows
df[df.ordqty < 0] = np.nan
df[df.price < 0] = np.nan
df.dropna(subset= ['ordqty'], axis= 0, inplace=True)

#Creating a normalized Order Qty column
df['norm_qty'] = df['ordqty'] / df['units']

#Creating a revenue column per item per day
df['revenue'] = df['shipqty'] * df['price']

#creating a column for the total orders per item
df['total_qty'] = df.groupby('item')["norm_qty"].transform('sum')

#creating a column for the total revenue per item
df['total_rev'] = df.groupby('item')["revenue"].transform('sum')

#splitting off the first phrase from the item's description, and turning them into lower case
df['label'] = df['desc'].str.split(",").str[0].str.lower()
df['label'] = df['label'].str.split(" ").str[0].str.lower()

#Figuring out all the low PLU quantities that were ordered ten times or less in a one year time frame

df['low_quantities'] = df['total_qty'].apply(lambda x: x if x > 10 else np.nan)
low= df[df['low_quantities'].isnull()]
print (f'Number of PLUs ordered less than 10 times last year = {low.item.nunique()}')
print ('List of all the PLUs ordered less than 10 times')
list(low.item.unique())

print (f'DF size before dropping the low quantity items = {df.shape[0]}')
print (f'Number of unique items(PLUs) before dropping the low quantity items = {df.item.nunique()}')
df.dropna(subset= ['low_quantities'], axis= 0, inplace=True)
print (f'DF size AFTER dropping the low quantity items = {df.shape[0]}')
print (f'Number of unique items(PLUs) AFTER dropping the low quantity items = {df.item.nunique()}')

"""*OK so we lost 768 items because they were too low of demand over a one years time period, but not suprisingly we did not loose too many entries in our invoice data since they represented so low of demand. We still have a lot of entries to work with.*"""

#quick visual of the demand of items after dropping the least ordered
plt.hist(df.total_qty, bins=50)

"""*So most of the demand is still centered around the zero mark which is good for a random forest which can handle the sparsity*"""

df.head(20)

#creating a data frame with just the normalized quantities ordered.
dfqty = pd.pivot_table(df, values= "norm_qty", index= "invdate", columns = "item", aggfunc=np.sum, fill_value=0)

# #Adding total columns--- dont think I want to add this...
# dfqty["day_total"]= dfqty.sum(axis = 1, skipna = True)

#resetting index to datetime
dfqty.index = pd.to_datetime(dfqty.index)

#filling in the missing days
idx = pd.date_range('2018-08-23', '2019-08-23')
dfqty = dfqty.reindex(idx, fill_value=0)

#Whoops accidentally did 366 days instead of 365! Let's fix that...
dfqty = dfqty[1:]

print (dfqty.shape)
dfqty.head()

dfwkly.index.unique()

dfqty.shape

#Creating a weekly DF

dfwkly = dfqty.resample('7D').sum()
print (dfwkly.shape)

#resetting index to datetime
dfwkly.index = pd.to_datetime(dfwkly.index)

#Whoops accidentally did 53 weeks instead of 52! Let's fix that...
dfwkly = dfwkly[:-1]

print (dfwkly.shape)

#melting the training data

dfwkly.reset_index(inplace= True) #reset index for melt
melt = dfwkly.melt(id_vars='index', var_name='item', value_name='sales')
melt = melt.sort_values(['index', 'item'])
dfwkly.set_index('index', inplace= True) #put index back
print (f'Number of Rows in the melted dataframe: {melt.shape}')
num_rows= melt.shape[0]
print (melt.head())

"""*Bueno! We have 486180 rows to work with!*

# Feature Engineering

We need to create lots of feature for the trees to sort with. After we run a couple test cases, we can also trim these down once we figure out which ones do not beneift the model. We will add the following:

* Date features like day number, week number month number, ect.
>* Day number of the week (0-6, where 0 = Monday, 6 =Sunday)
* Lag and differences of the current weeks demand (usually very correlated, and generally the more the merrier)
>* I will be taking the lags and differences in 7 day increments. Why? Because I want to model weekly demand, not daily. 
* Holiday Features
>* As discussed with the client that pertains to their type of business
"""

#Basic Feature engineering- Dates
melt2 = melt.copy()

melt2['date']= pd.to_datetime(melt2['index']) #converting index to a datetime
melt2.drop(columns= ['index'], inplace= True)


# Extracting date features
melt2['dayofmonth'] = melt2.date.dt.day
melt2['dayofyear'] = melt2.date.dt.dayofyear
melt2['dayofweek'] = melt2.date.dt.dayofweek
melt2['month'] = melt2.date.dt.month
melt2['year'] = melt2.date.dt.year
melt2['weekofyear'] = melt2.date.dt.weekofyear
melt2['is_month_start'] = (melt2.date.dt.is_month_start).astype(int)
melt2['is_month_end'] = (melt2.date.dt.is_month_end).astype(int)
melt2.head()

melt2.columns

melt2.head()

#Basic Feature Engineering- lags, differences, and logs-- These tend to help ALOT with time series data

melt2['last_wk_sales'] = melt2.groupby(['item'])['sales'].shift(1)
melt2['last_wk_diff'] = melt2.groupby(['item'])['sales'].diff(1)
melt2['log_sales'] = np.log1p(melt2.sales.values)# Converting sales to log(1+sales)
melt2['2wks_sales'] = melt2.groupby(['item'])['sales'].shift(2)
melt2['2wks_diff'] = melt2.groupby(['item'])['sales'].diff(2)
melt2['3wks_sales'] = melt2.groupby(['item'])['sales'].shift(3)
melt2['3wks_diff'] = melt2.groupby(['item'])['sales'].diff(3)
melt2['4wks_sales'] = melt2.groupby(['item'])['sales'].shift(4)
melt2['4wks_diff'] = melt2.groupby(['item'])['sales'].diff(4)
melt2['5wks_sales'] = melt2.groupby(['item'])['sales'].shift(5)
melt2['5wks_diff'] = melt2.groupby(['item'])['sales'].diff(5)
melt2['6wks_sales'] = melt2.groupby(['item'])['sales'].shift(6)
melt2['6wks_diff'] = melt2.groupby(['item'])['sales'].diff(6)
melt2['7wks_sales'] = melt2.groupby(['item'])['sales'].shift(7)
melt2['7wks_diff'] = melt2.groupby(['item'])['sales'].diff(7)
melt2['8wks_sales'] = melt2.groupby(['item'])['sales'].shift(7)
melt2['8wks_diff'] = melt2.groupby(['item'])['sales'].diff(7)
melt2 = melt2.dropna()

print (f'Number of Rows lost by differencing: {num_rows-melt2.shape[0]}') #how many rows did we loose from the differencing?
melt2.head()

#converting date to index datetime
melt2.date = pd.to_datetime(melt2.date)
melt2.set_index('date', inplace= True)

#Adding Holidays--- these were events discussed with the customer

#adding holidays--- just create a column with a bunch of True and Falses for each holiday
melt2["new_years"] = melt2.index.dayofyear == 1
melt2["christmas"] = (melt2.index.month == 12) & ((melt2.index.day >= 18)&(melt2.index.day <= 26))
melt2["thanksgiving"] = (melt2.index.month == 11) & ((melt2.index.day >= 22)&(melt2.index.day <= 27))
melt2["farm2table"] = (melt2.index.month == 7) & ((melt2.index.day >= 1)&(melt2.index.day <= 15))
melt2["memorial_day"] = (melt2.index.month == 5) & ((melt2.index.day >= 18)&(melt2.index.day <= 28))
melt2["back_to_school"] = (melt2.index.month == 9) & (melt2.index.day == 1)

melt2.head()

#I'm curious... how correlated are these variables to the actual sales?

# calculate the correlation matrix
corr = melt2.corr()

# plot the heatmap
plt.figure(figsize=(12,9))
sns.heatmap(corr, 
        xticklabels=corr.columns,
        yticklabels=corr.columns, cmap='magma')

plt.title("Correlation Matrix od the different features", fontsize= 18)

"""#### INSIGHTS


*   Anything close to pink is not great correlation. Anything on the lighter or darker side is a good correlation with white being the best positive correlation and black being the best negative correlation.
* Looks like there is a fair amount of correlation between past sales. 
*   Not a lot of correlation with the other variables
* Day number of the week and week number of the year have a sliiiiiight correlation. This makes sense because our EDA was showing some weekly trends.

# Re-establishing a Baseline and Train/Validation Set
"""

#rmse metric function
def rmse(ytrue, ypred):
  return (round(sqrt(mean_squared_error(ytrue, ypred)), 3))

#getting the last week of this subset of data
last_week = melt2['weekofyear'].iloc[-1]

#Restablishing a Naive Baseline and a Validation Split

mean_error = []
for week in range(last_week-8, last_week): #setting up eight weeks train/validation set
    train = melt2[melt2['weekofyear'] < week] 
    val = melt2[melt2['weekofyear'] == week]
    
    p = val['last_wk_sales'].values
    
    error = rmse(val['sales'].values, p)
    print('Week %d - Error %.5f' % (week, error))
    mean_error.append(error)
naive_error= np.mean(mean_error)
print('Mean Error = %.5f' % naive_error)

#You can SEE how sparse this data set is... how most of the demand is around 0
melt2['sales'].hist(bins=60, figsize=(10,6))
plt.title ("Overall, how sparse is the demand of the items?", fontsize= 16)

"""# Random Forest
* Before jumping straight into a Light Grandient Boost Model, lets first try a random forest.
* Random Forests are fantastic at being a great first model (out of the box) to throw at the problem.
* It has the ability to model multiple datatypes.
>* Later, I can play around with one-hot encoding on the categorical columns to see if that improves the score
* Parameters I will focus on= max number of trees, max depth of trees
"""

#dropping columns that the Random forest cannot compute
melt2.reset_index(inplace= True)
melt2.drop(columns= 'date', inplace=True)
melt2.set_index('item', inplace=True)
melt2.head()

mean_error = []

for week in range(last_week-8, last_week): #setting up 8 weeks train/test set
  
    train = melt2[melt2['weekofyear'] < week]
    val = melt2[melt2['weekofyear'] == week]
    
    xtr, xts = train.drop(['sales'], axis=1), val.drop(['sales'], axis=1)
    ytr, yts = train['sales'].values, val['sales'].values
    
    mdl = RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=0)
    mdl.fit(xtr, ytr)
    
    p = mdl.predict(xts)
    
    error = rmse(yts, p)
    print('Week %d - Error %.5f' % (week, error))
    mean_error.append(error)
    
print('Mean Error = %.5f' % np.mean(mean_error))

"""#### Is my model overfitting?


* If the training error is low and the validation error is much higher, your model is overfitting.
*   YES, empirically trees can definitely over fit.
*   One way to prevent overfitting is to trim the trees and indicate a max number of leaves. I will create a grid search, and then visualize the testing versus the training data and see at which values overfitting is occuring.
"""

#Is my model over fitting here? Possibly! Let's take a look at how max_depth affects overfitting
#We'll take the last week in the data set as the testing set and train on the other weeks

train = melt2[melt2['weekofyear'] < 33] #NOTE: our training set gets smaller the further out the weeks are
test = melt2[melt2['weekofyear'] == 33]
    
x_train, x_test = train.drop(['sales'], axis=1), test.drop(['sales'], axis=1)
y_train, y_test = train['sales'].values, test['sales'].values


max_depths = np.linspace(1, 15, 15, endpoint=True)
train_results = []
test_results = []

for max_depth in max_depths:
  rf = RandomForestRegressor(n_estimators=1000, max_depth=max_depth, n_jobs=-1)
  rf.fit(x_train, y_train)
  train_p = rf.predict(x_train)
  train_err = rmse(y_train, train_p)
  train_results.append(train_err)
  
  test_p = rf.predict(x_test)
  test_err = rmse(y_test, test_p)
  test_results.append(test_err)

#Plot Tree Depth of Testing Versus Training
plt.plot(max_depths, train_results, color= 'b', label='Train RMSE')
plt.plot(max_depths, test_results, color= 'r', label='Test RMSE')
plt.legend()
plt.ylabel('RMSE')
plt.ylim(0,1)
plt.xlabel('Tree Depth')
plt.show()

"""#### INSIGHTS
*GREAT! Tree depth of 5 seems to be a good trade-off between a lower RMSE and being able to genralize into the future. Although, all these values seem to be doing fairly well and not over fitting too badly.*
"""

# Commented out IPython magic to ensure Python compatibility.
# #Let's do the same thing for the max number of trees, and see what the optimal balance of lower RMSE without overfitting
# %%time
# 
# train = melt2[melt2['weekofyear'] < 33] #NOTE: our training set gets smaller the further out the weeks are
# test = melt2[melt2['weekofyear'] == 33]
#     
# x_train, x_test = train.drop(['sales'], axis=1), test.drop(['sales'], axis=1)
# y_train, y_test = train['sales'].values, test['sales'].values
# 
# 
# num_trees = [10000] #100, 500, 750, 1000, 2000
# train_results = []
# test_results = []
#  
# for num in num_trees:
#   rf = RandomForestRegressor(n_estimators=num, max_depth= 5, n_jobs=-1)
#   rf.fit(x_train, y_train)
#   train_p = rf.predict(x_train)
#   train_err = rmse(y_train, train_p)
#   train_results.append(train_err)
#   
#   test_p = rf.predict(x_test)
#   test_err = rmse(y_test, test_p)
#   test_results.append(test_err)
# 
# #Plot Tree Depth of Testing Versus Training
# plt.plot(num_trees, train_results, color= 'b', label='Train RMSE')
# plt.plot(num_trees, test_results, color= 'r', label='Test RMSE')
# plt.legend()
# plt.title("Best Max Number of Trees")
# plt.ylabel('RMSE')
# plt.xlabel('Max Number of Trees')
# plt.show()

print (train_results, test_results)

"""#### INSIGHTS


*   Hookay, between 10, 50, 100, 500, 1000 which number is the best?
*   1000 seems the be the optimal max number of trees which also helps with computing power

## Testing daily order qty for all the viable PLU codes...
"""

melt2 [melt2.weekofyear == 33]

# Commented out IPython magic to ensure Python compatibility.
# #Officially training and testing the last weeks model with the best tree parameters
#   
# #def model(df, week):
# 
# %%time 
# train = melt2[melt2['weekofyear'] < 33]
# test = melt2[melt2['weekofyear'] == 33]
#     
# x_train, x_test = train.drop(['sales'], axis=1), test.drop(['sales'], axis=1)
# y_train, y_test = train['sales'].values, test['sales'].values
#     
# mdl = RandomForestRegressor(n_estimators=1000, max_depth= 5, n_jobs=-1, random_state=0)
# mdl.fit(x_train, np.log1p(y_train))
#     
# p_train = np.expm1(mdl.predict(x_train))
# train_error = rmse(y_train, p_train) 
# print (f'Train Error= {train_error}')
# 
# pred_vs_actual_train = pd.DataFrame({
#     'actual': y_train,
#     'predicted': p_train,
#     'error_rf': (((p_train-y_train)/y_train*100)),
#     'rmse': train_error,
# })
# pred_vs_actual_train= pred_vs_actual_train.round(3)
#         
# p = np.expm1(mdl.predict(x_test))
# error = rmse(y_test, p)
# print (f'Test Error= {error}')
# 
# pred_vs_actual_test = pd.DataFrame({
#     'actual': y_test,
#     'predicted': p,
#     'error_rf': (((p-y_test)/y_test*100)),
#     'rmse': error,
# })
# pred_vs_actual_test= pred_vs_actual_test.round(3)

#Test
test.reset_index(inplace=True)
results_test = pd.concat([test, pred_vs_actual_test], axis=1, sort=False)
results_test= results_test[['item', 'weekofyear', 'actual', 'last_wk_sales','predicted', 'error_rf', 'rmse']]
results_test['error_n']= ((results_test.last_wk_sales.values- results_test.actual.values)/results_test.actual.values)*100

print (results_test)

#Actual values versus predicted values
print ("How much better did the Random Forest perform than the Naive Model?")
print(f'Random Forest model increased performance by: {abs((error-naive_error)/naive_error) * 100: .2f} % \n')
plt.figure(figsize=(10,6))
plt.scatter(results_test.actual, results_test.predicted, s=300, color='lightseagreen', alpha= .4, label= "Random Forest")
plt.scatter(results_test.actual, results_test.last_wk_sales, s=50, color='red', alpha= .1, label= "Naive Model")

x = np.linspace(0,60,60)
y = x
plt.plot(x, y, 'k', linestyle='--', label= "Perfect Prediction")
plt.legend()
plt.ylim(0,60)
plt.xlim(0,60)
plt.xlabel('Actual Quantities')
plt.ylabel('Predicted Quantities')
plt.title("How well did the Random Forest Do?", fontsize= 18)

"""#### Insights


*   Runs fairly slow, I can adjust number of trees to improve this and see how that affects the performance
*   62% increase in performance!
"""

feature_importances = pd.Series(mdl.feature_importances_, index=x_train.columns)
feature_importances.sort_values(inplace=True, ascending=False)
feature_importances[:50].plot.bar( color="dodgerblue", figsize= (10,8), width= 1)
plt.title("Most Important Features for Random Forest")

"""# Light Gradiant Boost Model
* Cutting edge with gradient boosting!
* Wins lots of Kaggle Competitions!
* Seems to be super efficient
"""

#LGBM WITH logged data

mean_error = []

for week in range(last_week-8, last_week): #setting up 8 weeks train/test set
  
    train = melt2[melt2['weekofyear'] < week]
    val = melt2[melt2['weekofyear'] == week]
    
    xtr, xts = train.drop(['sales'], axis=1), val.drop(['sales'], axis=1)
    ytr, yts = train['sales'].values, val['sales'].values
    
    mdl = LGBMRegressor(n_estimators=1000, learning_rate=0.01)
    mdl.fit(xtr, np.log1p(ytr))
    
    p = np.expm1(mdl.predict(xts))
    
    error = rmse(yts, p)
    print('Week %d - Error %.5f' % (week, error))
    mean_error.append(error)

print ("How Well Did the Models Do?\n")

print (f"Mean Error of Naive Model = {naive_error}")
print ("Mean Error of Random Forest= 0.289")
print(f"Mean Error of a LGBM = {np.mean(mean_error):.3f}" )

#parameter hyper tuning

# params = {
#     'boosting_type': 'gbdt',  # np.random.choice(['dart', 'gbdt']),
#     'objective': 'binary',
#     'metric': ['binary_logloss', 'auc'], 
    
#     'learning_rate': 0.35,
    
#     'num_leaves': np.random.randint(64, 128),
#     'max_depth': np.random.randint(6, 12),
#     'min_data_in_leaf': int(2 ** (np.random.rand()*3.5 + 9)),
    
#     'feature_fraction': np.random.rand()*0.35+0.65,
#     'bagging_fraction': np.random.rand()*0.35+0.65,
#     'bagging_freq': 1,
    
#     'lambda_l1': 10 ** (np.random.rand() * 4),
#     'lambda_l2': 10 ** (np.random.rand() * 3 + 2),
#     'min_gain_to_split': 0.0,
#     'min_sum_hessian_in_leaf': 0.1,
    
#     'num_threads': 16,
#     'verbose': 0,
#     'is_training_metric': 'True'
# }

# Hypertune Parameters of LGBM Model

from sklearn.model_selection import GridSearchCV
estimator = LGBMRegressor()

param_grid = {
    'learning_rate': [0.01, 0.05, .1, .2, .3],
    'n_estimators': [20, 50, 100, 500, 1000],
    'num_leaves':[3,4,5,6,7,8,9,10,15,20,25,30]
}

gbm = GridSearchCV(estimator, param_grid, cv=3)
gbm.fit(x_train, y_train)

print('Best parameters found by grid search are:', gbm.best_params_)

# Commented out IPython magic to ensure Python compatibility.
# #Is my model over fitting here? Possibly! Let's take a look at how max_depth affects overfitting
# #We'll take the last week in the data set as the testing set and train on the other weeks
# 
# %%time 
# train = melt2[melt2['weekofyear'] < 33] #NOTE: our training set gets smaller the further out the weeks are
# test = melt2[melt2['weekofyear'] == 33]
#     
# x_train, x_test = train.drop(['sales'], axis=1), test.drop(['sales'], axis=1)
# y_train, y_test = train['sales'].values, test['sales'].values
# 
# 
# max_depths = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
# train_results = []
# test_results = []
# 
# for max_depth in max_depths:
#   lgb = LGBMRegressor(n_estimators=1000, num_leaves=4, max_depth=max_depth, learning_rate=0.05)
#   lgb.fit(x_train, y_train)
#   train_p = lgb.predict(x_train)
#   train_err = rmse(y_train, train_p)
#   train_results.append(train_err)
#   
#   test_p = lgb.predict(x_test)
#   test_err = rmse(y_test, test_p)
#   test_results.append(test_err)
# 
# #Plot Tree Depth of Testing Versus Training
# plt.plot(max_depths, train_results, color= 'b', label='Train RMSE')
# plt.plot(max_depths, test_results, color= 'r', label='Test RMSE')
# plt.legend()
# plt.ylabel('RMSE')
# plt.ylim(0,1)
# plt.xlabel('Tree Depth')
# plt.show()

"""#### INSIGHTS


*   Best parameters found by grid search are: {'learning_rate': 0.05, 'n_estimators': 1000, 'num_leaves': 4}
*   To prevent overfitting max_depth is set to 2 to prevent overfitting
"""

# Commented out IPython magic to ensure Python compatibility.
# #Officially training and testing the last weeks model with the best tree parameters
#   
# #def model(df, week):
# 
# %%time 
# train = melt2[melt2['weekofyear'] < 34]
# test = melt2[melt2['weekofyear'] == 34]
#     
# x_train, x_test = train.drop(['sales'], axis=1), test.drop(['sales'], axis=1)
# y_train, y_test = train['sales'].values, test['sales'].values
#     
# lgb = LGBMRegressor(n_estimators=1000, num_leaves=4, max_depth= 2, learning_rate=0.05)
# lgb.fit(x_train, np.log1p(y_train))
#     
# p_train = np.expm1(lgb.predict(x_train))
# train_error = rmse(y_train, p_train) 
# print (f'Train Error= {train_error}')
# 
# pred_vs_actual_train = pd.DataFrame({
#     'actual': y_train,
#     'predicted': p_train,
#     'error_rf': (((p_train-y_train)/y_train*100)),
#     'rmse': train_error,
# })
# pred_vs_actual_train= pred_vs_actual_train.round(3)
#         
# p = np.expm1(lgb.predict(x_test))
# error = rmse(y_test, p)
# print (f'Test Error= {error}')
# 
# pred_vs_actual_test = pd.DataFrame({
#     'actual': y_test,
#     'predicted': p,
#     'error_rf': (((p-y_test)/y_test*100)),
#     'rmse': error,
# })
# pred_vs_actual_test= pred_vs_actual_test.round(3)

#Test
test.reset_index(inplace=True)
results_test = pd.concat([test, pred_vs_actual_test], axis=1, sort=False)
results_test= results_test[['item', 'weekofyear', 'actual', 'last_wk_sales','predicted', 'error_rf', 'rmse']]
results_test['error_n']= ((results_test.last_wk_sales.values- results_test.actual.values)/results_test.actual.values)*100

#Actual values versus predicted values
print ("How much better did the LightGBM perform than the Naive Model?")
print(f'LightGBM increased performance by: {abs((error-naive_error)/naive_error) * 100: .2f} % \n')
plt.figure(figsize=(10,6))
plt.scatter(results_test.actual, results_test.predicted, s=300, color='lightseagreen', alpha= .4, label= "LGBM Forest")
plt.scatter(results_test.actual, results_test.last_wk_sales, s=50, color='red', alpha= .1, label= "Naive Model")

x = np.linspace(0,60,60)
y = x
plt.plot(x, y, 'k', linestyle='--', label= "Perfect Prediction")
plt.legend()
plt.ylim(0,60)
plt.xlim(0,60)
plt.xlabel('Actual Quantities')
plt.ylabel('Predicted Quantities')
plt.title("How well did the LightGradientBoost Model Do?", fontsize= 18)

"""#### Which features are the most important?"""

# feature importances
feature_importances = pd.Series(lgb.feature_importances_, index=x_train.columns)
feature_importances.sort_values(inplace=True, ascending=False)
feature_importances[:50].plot.bar( color="dodgerblue", figsize= (10,8), width= 1)
plt.title("Most Important Features for LightGB MODEL")

"""# Questions

1. Is LGBM overfitting?
2.   Can I drop features that don't make a difference?
3.   Which models should I use?
4.  HOW TO TRANSLATE WHAT I HAVE TO ACTUAL WEEKLY PREDICTIONS????????????
    >* so i would have to create a data of x_variables somehow and then that would be my prediction for the y variables!
    >* how to create a data frame of x_variables?
5. HOW TO IMPLEMENT THIS MODEL IN THE REAL WORLD???
6.Can I implement every Sunday and Thursday?
7.best way to calculate accuracy for model?? percent change from naive RMSE to new RMSE????

# NOTES HOW TO REDUCE OVERFITTING

Tuning for overfitting

In addition to the parameters mentioned above the following parameters can be used to control overfitting:

* max_bin: the maximum numbers bins that feature values are bucketed in. A smaller max_bin reduces overfitting.
* min_child_weight: the minimum sum hessian for a leaf. In conjuction with min_child_samples, larger values reduce overfitting.
* bagging_fraction and bagging_freq: enables bagging (subsampling) of the training data. Both values need to be set for bagging to be used. The frequency controls how often (iteration) bagging is used. Smaller fractions and frequencies reduce overfitting.
* feature_fraction: controls the subsampling of features used for training (as opposed to subsampling the actual training data in the case of bagging). Smaller fractions reduce overfitting.
* lambda_l1 and lambda_l2: controls L1 and L2 regularization.
"""

